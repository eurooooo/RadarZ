from datetime import datetime
import os
from langgraph.types import Send
from .schemas import SearchQueryList, RelevanceAssessmentList
from .state import ResearchState, WebSearchState
from .prompts import query_writer_instructions, relevance_assessment_system_prompt
from langchain.chat_models import init_chat_model

from tavily import TavilyClient
from typing import Dict

def get_llm():
    return init_chat_model(model="gemini-2.5-flash", model_provider="google_genai", temperature=0)

def generate_queries(state: ResearchState) -> ResearchState:
    """ç”Ÿæˆ3ä¸ªä¸åŒç»´åº¦çš„æœç´¢æŸ¥è¯¢"""
    
    messages = query_writer_instructions.format_messages(
        project_name=state['project_name'],
        github_url=state['github_url'],
        readme_preview=state['readme'][:500]
    )

    llm = get_llm()
    
    response = llm.with_structured_output(SearchQueryList).invoke(messages)
    
    # return {'search_queries': response.query}
    return {'search_queries': [state['project_name']]}

def to_web_research(state: ResearchState):
    """LangGraph node that sends the search queries to the web research node.

    This is used to spawn n number of web research nodes, one for each search query.
    """
    return [
        Send("web_research", {"search_query": search_query})
        for search_query in state["search_queries"]
    ]

def web_research(state: WebSearchState) -> ResearchState:
    """å¤„ç†å•ä¸ªæœç´¢æŸ¥è¯¢ï¼ˆç”± Send å¹¶è¡Œè°ƒç”¨ï¼‰"""
    
    tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
    
    # ä» Send ä¼ é€’çš„ state ä¸­è·å–å•ä¸ªæŸ¥è¯¢
    query = state['search_query']
    
    print(f"ğŸ” Searching: {query}")
    
    # æ‰§è¡Œå•ä¸ªæŸ¥è¯¢çš„æœç´¢
    results = tavily_client.search(
        query=query,
        max_results=10,
        include_raw_content=True  # åŒ…å«å®Œæ•´ç½‘é¡µå†…å®¹
    )
    
    search_results = results.get('results', [])
    # åªè¿”å›åŸå§‹ç»“æœï¼ŒLangGraph ä¼šè‡ªåŠ¨åˆå¹¶ï¼ˆä½¿ç”¨ operator.addï¼‰
    return {
        'search_results': search_results
    }

def filter_irrelevant_results(state: ResearchState) -> ResearchState:
    """ç”¨ LLM è¿‡æ»¤æ‰ä¸ç›¸å…³çš„æœç´¢ç»“æœï¼ˆä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ç»“æœï¼‰"""
    
    # æ„å»ºæ‰€æœ‰æœç´¢ç»“æœçš„æ–‡æœ¬è¡¨ç¤º
    results = state['search_results']
    search_results_text = "\n\n---\n\n".join([
        f"Result {i+1}:\nTitle: {r['title']}\nURL: {r['url']}\nContent Preview:\n{(r.get('raw_content') or r.get('content') or 'None')[:1000]}"
        for i, r in enumerate(results)
    ])
    
    prompt = relevance_assessment_system_prompt.format_messages(
        project_name=state['project_name'],
        readme_preview=state['readme'][:500],
        search_results=search_results_text
    )
    
    print(f"ğŸ” Filtering {len(results)} results...")
    
    # ä¸€æ¬¡æ€§è¯„ä¼°æ‰€æœ‰ç»“æœ
    response = get_llm().with_structured_output(RelevanceAssessmentList).invoke(prompt)
    
    assessments = response.assessments
    
    # ç¡®ä¿è¯„ä¼°ç»“æœæ•°é‡ä¸æœç´¢ç»“æœæ•°é‡ä¸€è‡´
    if len(assessments) != len(results):
        print(f"âš ï¸ Warning: Expected {len(results)} assessments, got {len(assessments)}")
        # å¦‚æœæ•°é‡ä¸åŒ¹é…ï¼Œåªå¤„ç†åŒ¹é…çš„éƒ¨åˆ†
        min_len = min(len(assessments), len(results))
        assessments = assessments[:min_len]
        results = results[:min_len]
    
    # åªä¿ç•™ç›¸å…³çš„ç»“æœï¼ˆrelevance_score > 0.6ï¼‰
    filtered = []
    for result, assessment in zip(results, assessments):
        if assessment.is_relevant and assessment.relevance_score > 0.6:
            filtered.append(result)
            print(f"   âœ… {result['title'][:60]}... url: {result['url']} (score: {assessment.relevance_score:.2f})")
        else:
            print(f"   âŒ {result['title'][:60]}... url: {result['url']} (score: {assessment.relevance_score:.2f})")
    
    print(f"âœ… Filtered down to {len(filtered)} relevant results")
    
    return {'filtered_results': filtered}

def generate_final_summary(state: ResearchState) -> ResearchState:
    """åŸºäº README å’Œæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆæ€»ç»“"""
    
    llm = init_chat_model(
        model="claude-3-5-sonnet-20241022",
        model_provider="anthropic",
        temperature=0
    )
    
    # æ„å»ºæœç´¢ç»“æœçš„ä¸Šä¸‹æ–‡
    search_context = "\n\n---\n\n".join([
        f"Source: {r['title']}\nURL: {r['url']}\nRelevance: {r['relevance_score']:.2f}\n\nContent:\n{r['content'][:3000]}..."
        for r in state['filtered_results'][:10]  # æœ€å¤šç”¨å‰10ä¸ªç»“æœ
    ])
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a technical writer creating a comprehensive project summary.
        
        Your summary should include:
        1. **Overview**: What the project does (1-2 sentences)
        2. **Key Features**: Main functionalities (bullet points)
        3. **Technical Highlights**: Interesting technical approaches or innovations
        4. **Real-World Reception**: What users/community think (based on search results)
        5. **Pros**: Strengths of the project
        6. **Cons**: Limitations or issues (if any mentioned)
        7. **Comparison**: How it differs from alternatives (if discussed)
        8. **Use Cases**: Who should use this and when
        
        Be objective and cite sources when making claims.
        Use markdown formatting."""),
        
        ("user", """Project: {project_name}
        GitHub: {github_url}
        Stars: {stars} | Language: {language}
        
        === PROJECT README ===
        {readme}
        
        === EXTERNAL RESEARCH (from web search) ===
        {search_context}
        
        Generate a comprehensive summary:""")
    ])
    
    print("ğŸ“ Generating final summary...")
    
    response = llm.invoke(prompt.format_messages(
        project_name=state['project_name'],
        github_url=state['github_url'],
        stars=state['repo_stats'].get('stars', 'N/A'),
        language=state['repo_stats'].get('language', 'Unknown'),
        readme=state['readme'][:5000],  # README é™åˆ¶é•¿åº¦é¿å…è¶… token
        search_context=search_context
    ))
    
    state['final_summary'] = response.content
    print("âœ… Summary generated!")
    
    return state