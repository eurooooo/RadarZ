from datetime import datetime
import os
from pathlib import Path
from langgraph.types import Send
from .schemas import FinalSummary, SearchQueryList, RelevanceAssessmentList
from .state import ResearchState, WebSearchState
from .prompts import query_writer_instructions, relevance_assessment_system_prompt, final_summary_prompt, test_summary_prompt
from langchain.chat_models import init_chat_model

from tavily import TavilyClient

MODEL_NAME = os.getenv("MODEL_NAME")
MODEL_PROVIDER = os.getenv("MODEL_PROVIDER") or None

def get_llm():
    return init_chat_model(model=MODEL_NAME, model_provider=MODEL_PROVIDER, temperature=0)
    
def generate_queries(state: ResearchState) -> ResearchState:
    """ç”Ÿæˆæœç´¢æŸ¥è¯¢"""
    
    messages = query_writer_instructions.format_messages(
        project_name=state['project_name'],
        readme=state['readme']
    )

    llm = get_llm()
    
    response = llm.with_structured_output(SearchQueryList).invoke(messages)

    print(f"ğŸ” Generated search queries: {response.query}")

    return {'search_queries': response.query}
    # return {'search_queries': [state['project_name']]}

def to_web_research(state: ResearchState):
    """LangGraph node that sends the search queries to the web research node.

    This is used to spawn n number of web research nodes, one for each search query.
    """
    return [
        Send("web_research", {"search_query": search_query})
        for search_query in state["search_queries"]
    ]

def web_research(state: WebSearchState) -> ResearchState:
    """å¤„ç†å•ä¸ªæœç´¢æŸ¥è¯¢ï¼ˆç”± Send å¹¶è¡Œè°ƒç”¨ï¼‰"""
    
    tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
    
    # ä» Send ä¼ é€’çš„ state ä¸­è·å–å•ä¸ªæŸ¥è¯¢
    query = state['search_query']
    
    print(f"ğŸ” Searching: {query}")
    
    # æ‰§è¡Œå•ä¸ªæŸ¥è¯¢çš„æœç´¢
    results = tavily_client.search(
        query=query,
        max_results=10,
        include_raw_content=True  # åŒ…å«å®Œæ•´ç½‘é¡µå†…å®¹
    )
    
    search_results = results.get('results', [])
    # åªè¿”å›åŸå§‹ç»“æœï¼ŒLangGraph ä¼šè‡ªåŠ¨åˆå¹¶ï¼ˆä½¿ç”¨ operator.addï¼‰
    return {
        'search_results': search_results
    }

def filter_irrelevant_results(state: ResearchState) -> ResearchState:
    """ç”¨ LLM è¿‡æ»¤æ‰ä¸ç›¸å…³çš„æœç´¢ç»“æœï¼ˆä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ç»“æœï¼‰"""
    
    # æ„å»ºæ‰€æœ‰æœç´¢ç»“æœçš„æ–‡æœ¬è¡¨ç¤º
    results = state['search_results']
    search_results_text = "\n\n---\n\n".join([
        f"Result {i+1}:\nTitle: {r['title']}\nURL: {r['url']}\nContent Preview:\n{(r.get('raw_content') or r.get('content') or 'None')[:1000]}"
        for i, r in enumerate(results)
    ])
    
    prompt = relevance_assessment_system_prompt.format_messages(
        project_name=state['project_name'],
        readme=state['readme'],
        search_results=search_results_text
    )
    
    print(f"ğŸ” Filtering {len(results)} results...")
    
    # ä¸€æ¬¡æ€§è¯„ä¼°æ‰€æœ‰ç»“æœ
    response = get_llm().with_structured_output(RelevanceAssessmentList).invoke(prompt)
    
    assessments = response.assessments
    
    # ç¡®ä¿è¯„ä¼°ç»“æœæ•°é‡ä¸æœç´¢ç»“æœæ•°é‡ä¸€è‡´
    if len(assessments) != len(results):
        print(f"âš ï¸ Warning: Expected {len(results)} assessments, got {len(assessments)}")
        # å¦‚æœæ•°é‡ä¸åŒ¹é…ï¼Œåªå¤„ç†åŒ¹é…çš„éƒ¨åˆ†
        min_len = min(len(assessments), len(results))
        assessments = assessments[:min_len]
        results = results[:min_len]
    
    # åªä¿ç•™ç›¸å…³çš„ç»“æœï¼ˆrelevance_score > 0.6ï¼‰
    filtered = []
    for result, assessment in zip(results, assessments):
        if assessment.is_relevant and assessment.relevance_score > 0.6:
            filtered.append(result)
            print(f"   âœ… {result['title'][:60]}... url: {result['url']} (score: {assessment.relevance_score:.2f})")
        else:
            print(f"   âŒ {result['title'][:60]}... url: {result['url']} (score: {assessment.relevance_score:.2f})")
    
    print(f"âœ… Filtered down to {len(filtered)} relevant results")
    
    return {'filtered_results': filtered}

def generate_final_summary(state: ResearchState) -> ResearchState:
    """åŸºäº README å’Œæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆæ€»ç»“"""
    
    # æ„å»ºæœç´¢ç»“æœçš„æ–‡æœ¬è¡¨ç¤º
    filtered_results = state.get('filtered_results', [])
    
    if filtered_results:
        results_text = "\n\n---\n\n".join([
            f"æ ‡é¢˜: {r.get('title', 'N/A')}\n"
            f"URL: {r.get('url', 'N/A')}\n"
            f"å†…å®¹: {(r.get('raw_content') or r.get('content') or 'N/A')[:2000]}"
            for r in filtered_results
        ])
    else:
        results_text = "æœªæ‰¾åˆ°ç›¸å…³çš„æœç´¢ç»“æœã€‚"
    
    # æ„å»º prompt
    messages = final_summary_prompt.format_messages(
        project_name=state['project_name'],
        readme=state['readme'],
        filtered_results=results_text
    )
    
    print(f"ğŸ“ Generating final summary for {state['project_name']}...")
    
    # è°ƒç”¨ LLM ç”Ÿæˆæ€»ç»“
    llm = get_llm()
    response = llm.with_structured_output(FinalSummary).invoke(messages)
    
    final_summary = response.summary
    
    print(f"âœ… Final summary generated ({len(final_summary)} characters)")
    
    # å°†æ€»ç»“å†™å…¥æ–‡ä»¶
    output_dir = Path(__file__).parent.parent.parent / "summaries"
    output_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼ˆå¤„ç†ç‰¹æ®Šå­—ç¬¦å¦‚ /ï¼‰
    safe_project_name = state['project_name'].replace("/", "_").replace("\\", "_")
    safe_project_name = "".join(c for c in safe_project_name if c.isalnum() or c in ('_', '-', '.'))
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{safe_project_name}_{timestamp}.md"
    filepath = output_dir / filename
    
    # å†™å…¥æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(f"# {state['project_name']}\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")
        f.write(final_summary)
    
    print(f"ğŸ’¾ Summary saved to: {filepath}")
    
    return {'final_summary': final_summary}

def test_generate_final_summary(state: ResearchState) -> ResearchState:
    """åŸºäº README ç”Ÿæˆæœ€ç»ˆæ€»ç»“ï¼ˆæµ‹è¯•ç”¨ï¼‰"""
    
    # æ„å»º prompt
    messages = test_summary_prompt.format_messages(
        project_name=state['project_name'],
        readme=state['readme']
    )
    
    print(f"ğŸ“ Generating test summary for {state['project_name']}...")
    
    llm = get_llm()
    response = llm.invoke(messages)
    
    final_summary = response.content if hasattr(response, 'content') else str(response)
    
    print(f"âœ… Test summary generated ({len(final_summary)} characters)")
    
    # å°†æ€»ç»“å†™å…¥æ–‡ä»¶ï¼ˆæµ‹è¯•ç‰ˆæœ¬ï¼‰
    output_dir = Path(__file__).parent.parent.parent / "summaries"
    output_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼ˆå¤„ç†ç‰¹æ®Šå­—ç¬¦å¦‚ /ï¼‰ï¼Œå¹¶æ·»åŠ  test æ ‡è¯†
    safe_project_name = state['project_name'].replace("/", "_").replace("\\", "_")
    safe_project_name = "".join(c for c in safe_project_name if c.isalnum() or c in ('_', '-', '.'))
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{safe_project_name}_test_{timestamp}.md"
    filepath = output_dir / filename
    
    # å†™å…¥æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(f"# {state['project_name']} (æµ‹è¯•ç‰ˆ)\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("**æ³¨æ„ï¼š** è¿™æ˜¯åŸºäº README çš„æµ‹è¯•ç‰ˆæœ¬æ€»ç»“ï¼ŒæœªåŒ…å«ç½‘ç»œæœç´¢ç»“æœã€‚\n\n")
        f.write("---\n\n")
        f.write(final_summary)
    
    print(f"ğŸ’¾ Test summary saved to: {filepath}")
    
    return {'final_summary': final_summary}