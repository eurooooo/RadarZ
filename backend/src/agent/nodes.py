from datetime import datetime
import os
from langgraph.types import Send
from .schemas import SearchQueryList
from .state import ResearchState, WebSearchState
from .prompts import query_writer_instructions
from langchain.chat_models import init_chat_model

from tavily import TavilyClient
from typing import Dict

def generate_queries(state: ResearchState) -> ResearchState:
    """ç”Ÿæˆ3ä¸ªä¸åŒç»´åº¦çš„æœç´¢æŸ¥è¯¢"""
    
    llm = init_chat_model(model="gemini-2.5-flash", model_provider="google_genai", temperature=0)
    
    prompt = query_writer_instructions.format(
        project_name=state['project_name'],
        github_url=state['github_url'],
        readme_preview=state['readme'][:500]
    )
    
    response = llm.with_structured_output(SearchQueryList).invoke(prompt)
    
    return {'search_queries': response.query}

def to_web_research(state: ResearchState):
    """LangGraph node that sends the search queries to the web research node.

    This is used to spawn n number of web research nodes, one for each search query.
    """
    return [
        Send("web_research", {"search_query": search_query})
        for search_query in state["search_queries"]
    ]

def web_research(state: WebSearchState) -> ResearchState:
    """å¤„ç†å•ä¸ªæœç´¢æŸ¥è¯¢ï¼ˆç”± Send å¹¶è¡Œè°ƒç”¨ï¼‰"""
    
    tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
    
    # ä» Send ä¼ é€’çš„ state ä¸­è·å–å•ä¸ªæŸ¥è¯¢
    query = state['search_query']
    
    print(f"ğŸ” Searching: {query}")
    
    # æ‰§è¡Œå•ä¸ªæŸ¥è¯¢çš„æœç´¢
    results = tavily_client.search(
        query=query,
        search_depth="advanced",  # è·å–å®Œæ•´å†…å®¹
        max_results=10,
        include_raw_content=True  # åŒ…å«å®Œæ•´ç½‘é¡µå†…å®¹
    )
    
    search_results = {
        'query': results.get('query'),
        'title': results.get('results')[0].get('title'),
    }
    # åªè¿”å›åŸå§‹ç»“æœï¼ŒLangGraph ä¼šè‡ªåŠ¨åˆå¹¶ï¼ˆä½¿ç”¨ operator.addï¼‰
    return {
        'raw_search_results': [search_results]
    }

def aggregate_and_deduplicate(state: ResearchState) -> ResearchState:
    """èšåˆæ‰€æœ‰æœç´¢ç»“æœå¹¶å»é‡"""
    from urllib.parse import urlparse
    
    # å±•å¹³æ‰€æœ‰ç»“æœï¼ˆå› ä¸º operator.add å¯èƒ½äº§ç”ŸåµŒå¥—åˆ—è¡¨ï¼‰
    flat_results = []
    raw_results = state.get('raw_search_results', [])
    
    # å¤„ç† raw_search_resultsï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨çš„åˆ—è¡¨ï¼Œå› ä¸º operator.add ä¼šåˆå¹¶åˆ—è¡¨ï¼‰
    if raw_results:
        for item in raw_results:
            if isinstance(item, list):
                flat_results.extend(item)
            else:
                flat_results.append(item)
    
    print(f"ğŸ“Š Total search results before deduplication: {len(flat_results)}")
    
    # å»é‡é€»è¾‘
    seen_urls = set()
    deduplicated = []
    
    for result in flat_results:
        url = result.get('url', '')
        
        # è§„èŒƒåŒ– URLï¼ˆå»æ‰ query params å’Œ fragmentsï¼‰
        parsed = urlparse(url)
        normalized_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        
        if normalized_url and normalized_url not in seen_urls:
            seen_urls.add(normalized_url)
            deduplicated.append({
                'url': result.get('url', ''),
                'title': result.get('title', ''),
                'content': result.get('raw_content', result.get('content', '')),
                'score': result.get('score', 0)
            })
    
    print(f"âœ… After deduplication: {len(deduplicated)} unique results")
    
    return {'deduplicated_results': deduplicated}

from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
import asyncio
class RelevanceAssessment(BaseModel):
    """ç›¸å…³æ€§è¯„ä¼°ç»“æœ"""
    is_relevant: bool = Field(description="æ˜¯å¦ä¸é¡¹ç›®ç›¸å…³")
    relevance_score: float = Field(description="ç›¸å…³æ€§åˆ†æ•° 0-1")
    reason: str = Field(description="åˆ¤æ–­ç†ç”±")

async def filter_irrelevant_results(state: ResearchState) -> ResearchState:
    """ç”¨ LLM è¿‡æ»¤æ‰ä¸ç›¸å…³çš„æœç´¢ç»“æœ"""
    
    llm = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        temperature=0
    ).with_structured_output(RelevanceAssessment)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a relevance assessment expert. 
        Evaluate if a search result is relevant to understanding a GitHub project.
        
        Relevant results include:
        - Reviews, opinions, or comparisons of the project
        - Real-world usage examples or case studies
        - Technical discussions or tutorials
        - Issues, limitations, or criticisms
        
        Irrelevant results include:
        - Completely unrelated topics
        - Generic programming tutorials not specific to this project
        - Spam or low-quality content"""),
        
        ("user", """Project: {project_name}
        README: {readme_preview}
        
        Search Result:
        Title: {title}
        URL: {url}
        Content Preview (first 1000 chars):
        {content_preview}
        
        Assess relevance:""")
    ])
    
    # å¹¶è¡Œè¯„ä¼°æ‰€æœ‰ç»“æœ
    async def assess_single_result(result: Dict) -> tuple[Dict, RelevanceAssessment]:
        assessment = await llm.ainvoke(prompt.format_messages(
            project_name=state['project_name'],
            readme_preview=state['readme'][:500],
            title=result['title'],
            url=result['url'],
            content_preview=result['content'][:1000]
        ))
        return result, assessment
    
    print(f"ğŸ” Filtering {len(state['deduplicated_results'])} results...")
    
    tasks = [assess_single_result(r) for r in state['deduplicated_results']]
    assessments = await asyncio.gather(*tasks)
    
    # åªä¿ç•™ç›¸å…³çš„ç»“æœï¼ˆrelevance_score > 0.6ï¼‰
    filtered = []
    for result, assessment in assessments:
        if assessment.is_relevant and assessment.relevance_score > 0.6:
            result['relevance_score'] = assessment.relevance_score
            result['relevance_reason'] = assessment.reason
            filtered.append(result)
            print(f"   âœ… {result['title'][:60]}... (score: {assessment.relevance_score:.2f})")
        else:
            print(f"   âŒ {result['title'][:60]}... (score: {assessment.relevance_score:.2f})")
    
    # æŒ‰ç›¸å…³æ€§æ’åº
    filtered.sort(key=lambda x: x['relevance_score'], reverse=True)
    
    print(f"âœ… Filtered down to {len(filtered)} relevant results")
    
    state['filtered_results'] = filtered
    return state

def generate_final_summary(state: ResearchState) -> ResearchState:
    """åŸºäº README å’Œæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆæ€»ç»“"""
    
    llm = ChatAnthropic(model="claude-3-5-sonnet-20241022", temperature=0)
    
    # æ„å»ºæœç´¢ç»“æœçš„ä¸Šä¸‹æ–‡
    search_context = "\n\n---\n\n".join([
        f"Source: {r['title']}\nURL: {r['url']}\nRelevance: {r['relevance_score']:.2f}\n\nContent:\n{r['content'][:3000]}..."
        for r in state['filtered_results'][:10]  # æœ€å¤šç”¨å‰10ä¸ªç»“æœ
    ])
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a technical writer creating a comprehensive project summary.
        
        Your summary should include:
        1. **Overview**: What the project does (1-2 sentences)
        2. **Key Features**: Main functionalities (bullet points)
        3. **Technical Highlights**: Interesting technical approaches or innovations
        4. **Real-World Reception**: What users/community think (based on search results)
        5. **Pros**: Strengths of the project
        6. **Cons**: Limitations or issues (if any mentioned)
        7. **Comparison**: How it differs from alternatives (if discussed)
        8. **Use Cases**: Who should use this and when
        
        Be objective and cite sources when making claims.
        Use markdown formatting."""),
        
        ("user", """Project: {project_name}
        GitHub: {github_url}
        Stars: {stars} | Language: {language}
        
        === PROJECT README ===
        {readme}
        
        === EXTERNAL RESEARCH (from web search) ===
        {search_context}
        
        Generate a comprehensive summary:""")
    ])
    
    print("ğŸ“ Generating final summary...")
    
    response = llm.invoke(prompt.format_messages(
        project_name=state['project_name'],
        github_url=state['github_url'],
        stars=state['repo_stats'].get('stars', 'N/A'),
        language=state['repo_stats'].get('language', 'Unknown'),
        readme=state['readme'][:5000],  # README é™åˆ¶é•¿åº¦é¿å…è¶… token
        search_context=search_context
    ))
    
    state['final_summary'] = response.content
    print("âœ… Summary generated!")
    
    return state